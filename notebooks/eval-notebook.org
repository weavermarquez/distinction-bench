#+title: Distinction Benchmark
#+date: <2025-12-12 Fri>
#+author: Valerie Kim
#+property: header-args:python :session lof :results output :python "../.venv/bin/python"

* Introduction

This notebook evaluates whether Large Language Models can correctly simplify
Laws of Form (LoF) arithmetic expressions using Spencer-Brown's two axioms.

** The Calculus of Indications

George Spencer-Brown's /Laws of Form/ (1969) introduces a minimal calculus
based on a single symbol: the /mark/ or /cross/, written as =()=.

The calculus has only two axioms:

| Axiom | Name    | Rule           | Interpretation      |
|-------+---------+----------------+---------------------|
| I1    | Number  | =()()= → =()=  | Condense/Confirm    |
| I2    | Order   | =(())= → void  | Cancel/Compensate   |

- *I1 (Number)*: Multiple adjacent marks condense to a single mark
- *I2 (Order)*: A mark containing only a mark cancels to void (nothing)

Every well-formed expression reduces to exactly one of two values:
- =()= (the mark) — equivalent to TRUE or 1
- void (empty) — equivalent to FALSE or 0

* Setup

#+begin_src python
import os
import json
import re
import asyncio
from datetime import datetime
from dataclasses import dataclass, field
from collections import defaultdict
from dotenv import load_dotenv

from lofbench import (
    canonical_string,
    generate_test_cases,
    generate_composite_test_cases,
)

load_dotenv()

@dataclass
class Config:
    """Central configuration for all evaluations."""
    openai_key: str = field(default_factory=lambda: os.environ.get("OPENAI_API_KEY", ""))
    anthropic_key: str = field(default_factory=lambda: os.environ.get("ANTHROPIC_API_KEY", ""))
    gemini_key: str = field(default_factory=lambda: os.environ.get("GEMINI_API_KEY", ""))

    models: dict = field(default_factory=lambda: {
        "openai": "gpt-5.1",
        "anthropic": "claude-opus-4-5-20251101",
        "google": "gemini-3-pro-preview"
    })

    sample_size: int = 5
    composite_group_size: int = 8
    seed: int = 2024

    @property
    def available_providers(self) -> list[str]:
        providers = []
        if self.openai_key: providers.append("openai")
        if self.anthropic_key: providers.append("anthropic")
        if self.gemini_key: providers.append("google")
        return providers

CFG = Config()

print("Setup complete.")
for p in ["openai", "anthropic", "google"]:
    key = getattr(CFG, f"{p}_key" if p != "google" else "gemini_key")
    status = "✓" if key else "✗"
    print(f"  {status} {p}: {CFG.models[p]}")
#+end_src

#+RESULTS:
: Setup complete.
:   ✓ openai: gpt-5.1
:   ✓ anthropic: claude-opus-4-5-20251101
:   ✓ google: gemini-3-pro-preview

* Generate Test Cases

#+begin_src python
test_cases = generate_test_cases(n=4000, seed=20251211)
composite_cases = generate_composite_test_cases(n_groups=500, group_size=8, seed=2024)

print(f"Generated {len(test_cases)} single test cases")
print(f"Generated {len(composite_cases)} composite test cases")

# Save
with open("test_cases.json", "w") as f:
    json.dump(test_cases, f, indent=2)
with open("composite_test_cases.json", "w") as f:
    json.dump(composite_cases, f, indent=2)
#+end_src

* Load Test Cases

#+begin_src python
with open("test_cases.json", "r") as f:
    test_cases = json.load(f)
with open("composite_test_cases.json", "r") as f:
    composite_cases = json.load(f)
print(f"Loaded {len(test_cases)} single, {len(composite_cases)} composite test cases")
#+end_src

* Prompts

#+begin_src python
PROMPT_TEMPLATE = """You are an expert in evaluating Laws of Form expressions. Your task is to analyze an expression that represents a structure of distinctions and reduce it to its simplest form using two fundamental axioms.

Here is the expression you need to evaluate:

<expression>
{expression}
</expression>

## The form and axioms

#### Axiom 1. The law of calling
Multiple adjacent boundaries with nothing else inside them condense into one.
Example: ()() = ()

#### Axiom 2. The law of crossing
Two nested boundaries with nothing else between them annihilate to void.
Example: (()) = void

## Your Approach

1. Identify the structure
2. Look for opportunities to apply axioms (I1 or I2)
3. Apply axioms iteratively until no more reductions are possible
4. State the final form

After completing your reduction, provide your final answer:

<answer>X</answer>

where X is either:
- unmarked (if the expression reduces to void)
- marked (if structure remains)
"""

COMPOSITE_PROMPT_TEMPLATE = """You are an expert in evaluating Laws of Form expressions.

Here are the expressions you need to evaluate:

<expressions>
{expressions}
</expressions>

#### Axiom 1. The law of calling
Multiple adjacent boundaries condense into one: ()() = ()

#### Axiom 2. The law of crossing
Two nested boundaries annihilate to void: (()) = void

For each expression, reduce it and determine if it's marked or unmarked.

<answer>
E1: marked/unmarked
E2: marked/unmarked
...
total_marked: N
</answer>
"""


def extract_answer(response: str) -> str:
    """Extract the final answer from LLM response. Returns '()' or 'void'."""
    if response is None:
        return "unknown"

    match = re.search(r'<answer>\s*(marked|unmarked|\(\)|void)\s*</answer>', response, re.IGNORECASE)
    if match:
        ans = match.group(1).lower()
        return "()" if ans in ("marked", "()") else "void"

    response_lower = response.lower()
    mark_pos = max(response_lower.rfind("()"), response_lower.rfind("marked"))
    void_pos = max(response_lower.rfind("void"), response_lower.rfind("unmarked"))

    if mark_pos > void_pos:
        return "()"
    elif void_pos > mark_pos:
        return "void"
    return "unknown"


def extract_composite_answer(response: str, n: int) -> int:
    """Extract the count from LLM response. Returns -1 if unparseable."""
    if response is None:
        return -1

    match = re.search(r'<answer>.*?total_marked:\s*(\d+).*?</answer>', response, re.IGNORECASE | re.DOTALL)
    if match:
        val = int(match.group(1))
        return val if 0 <= val <= n else -1

    match = re.search(r'(?:count|total)[_:\s]+(\d+)', response, re.IGNORECASE)
    if match:
        val = int(match.group(1))
        return val if 0 <= val <= n else -1

    return -1
#+end_src

* Eval Framework

#+begin_src python
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from google import genai


class SingleTask:
    @staticmethod
    def format_prompt(case: dict) -> str:
        return PROMPT_TEMPLATE.format(expression=case["input"])

    @staticmethod
    def extract(response: str, case: dict):
        return extract_answer(response)

    @staticmethod
    def is_correct(answer, case: dict) -> bool:
        return answer == case["target"]

    @staticmethod
    def make_result(case: dict, provider: str, model: str, response: str, answer) -> dict:
        return {
            "id": case["id"],
            "input": case["input"],
            "target": case["target"],
            "difficulty": case["difficulty"],
            "provider": provider,
            "model": model,
            "response": response,
            "extracted_answer": answer,
            "correct": SingleTask.is_correct(answer, case)
        }


class CompositeTask:
    @staticmethod
    def format_prompt(case: dict) -> str:
        lines = [f"{i}. {expr}" for i, expr in enumerate(case["expressions"], 1)]
        return COMPOSITE_PROMPT_TEMPLATE.format(expressions="\n".join(lines))

    @staticmethod
    def extract(response: str, case: dict) -> int:
        return extract_composite_answer(response, case["group_size"])

    @staticmethod
    def is_correct(answer: int, case: dict) -> bool:
        return answer == case["count"]

    @staticmethod
    def make_result(case: dict, provider: str, model: str, response: str, answer: int) -> dict:
        return {
            "id": case["id"],
            "expressions": case["expressions"],
            "targets": case["targets"],
            "target_count": case["count"],
            "difficulty": case["difficulty"],
            "provider": provider,
            "model": model,
            "response": response,
            "extracted_answer": answer,
            "correct": CompositeTask.is_correct(answer, case)
        }


async def eval_openai(prompt: str, model: str) -> str:
    async with AsyncOpenAI() as client:
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
    return response.choices[0].message.content


async def eval_anthropic(prompt: str, model: str) -> str:
    client = AsyncAnthropic()
    async with client.messages.stream(
        model=model,
        max_tokens=16000,
        messages=[{"role": "user", "content": prompt}]
    ) as stream:
        message = await stream.get_final_message()
    return message.content[0].text


async def eval_google(prompt: str, model: str) -> str:
    async with genai.Client().aio as client:
        response = await client.models.generate_content(model=model, contents=prompt)
    return response.text


EVAL_FNS = {
    "openai": eval_openai,
    "anthropic": eval_anthropic,
    "google": eval_google
}


async def run_eval(cases, task_type=SingleTask, providers=None, n=None):
    if providers is None:
        providers = CFG.available_providers
    if n is not None:
        cases = cases[:n]

    tasks = []
    task_info = []

    for case in cases:
        prompt = task_type.format_prompt(case)
        for provider in providers:
            model = CFG.models[provider]
            fn = EVAL_FNS[provider]
            tasks.append(fn(prompt, model))
            task_info.append((case, provider, model))

    responses = await asyncio.gather(*tasks, return_exceptions=True)

    results = []
    for (case, provider, model), resp in zip(task_info, responses):
        if isinstance(resp, Exception):
            response, answer = f"ERROR: {resp}", None
        else:
            response = resp
            answer = task_type.extract(response, case)
        results.append(task_type.make_result(case, provider, model, response, answer))

    return results


def quick_test(cases, task_type=SingleTask, n=None):
    n = n or CFG.sample_size
    print(f"Running {task_type.__name__} on {n} cases...")
    results = asyncio.run(run_eval(cases, task_type, n=n))

    correct = sum(1 for r in results if r["correct"])
    print(f"\nResults: {correct}/{len(results)} ({100*correct/len(results):.1f}%)")

    for r in results:
        status = "✓" if r["correct"] else "✗"
        if task_type == SingleTask:
            print(f"  {status} {r['id']} ({r['provider']}): {r['input'][:25]}... → {r['extracted_answer']}")
        else:
            print(f"  {status} {r['id']} ({r['provider']}): count={r['extracted_answer']} (target: {r['target_count']})")

    return results


def analyze(results):
    is_composite = "target_count" in results[0] if results else False

    by_model = defaultdict(lambda: {"correct": 0, "total": 0, "off_by": []})
    by_diff = defaultdict(lambda: defaultdict(lambda: {"correct": 0, "total": 0}))

    for r in results:
        model = r["model"]
        by_model[model]["total"] += 1
        if r["correct"]:
            by_model[model]["correct"] += 1
        elif is_composite and r["extracted_answer"] is not None and r["extracted_answer"] >= 0:
            off = abs(r["extracted_answer"] - r["target_count"])
            by_model[model]["off_by"].append(off)

        by_diff[model][r["difficulty"]]["total"] += 1
        if r["correct"]:
            by_diff[model][r["difficulty"]]["correct"] += 1

    return {
        "by_model": {m: {
            "accuracy": d["correct"]/d["total"] if d["total"] > 0 else 0,
            "correct": d["correct"],
            "total": d["total"],
        } for m, d in by_model.items()},
        "by_difficulty": {m: {diff: {
            "accuracy": d["correct"]/d["total"] if d["total"] > 0 else 0,
        } for diff, d in diffs.items()} for m, diffs in by_diff.items()},
    }

print("Eval framework loaded.")
#+end_src

* Run Evaluation

#+begin_src python :async
# Quick test
# results = quick_test(test_cases, SingleTask, n=5)
# results = quick_test(composite_cases, CompositeTask, n=5)

# Full run
results = asyncio.run(run_eval(composite_cases, CompositeTask))

# Save
with open("results.json", "w") as f:
    json.dump(results, f, indent=2)
#+end_src

* Analyze Results

#+begin_src python
with open("results.json", "r") as f:
    results = json.load(f)

analysis = analyze(results)

print("=== Accuracy by Model ===")
for model, stats in analysis["by_model"].items():
    pct = stats["accuracy"] * 100
    print(f"{model}: {pct:.1f}% ({stats['correct']}/{stats['total']})")
#+end_src
