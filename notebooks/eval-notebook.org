#+title: Distinction Benchmark
#+date: <2025-12-12 Fri>
#+author: Valerie Kim
#+property: header-args:python :session lof :results output :python "../.venv/bin/python"

* Introduction

This notebook evaluates whether Large Language Models can correctly simplify
Laws of Form (LoF) arithmetic expressions using Spencer-Brown's two axioms.

** The Calculus of Indications

George Spencer-Brown's /Laws of Form/ (1969) introduces a minimal calculus
based on a single symbol: the /mark/ or /cross/, written as =()=.

The calculus has only two axioms:

| Axiom | Name    | Rule           | Interpretation      |
|-------+---------+----------------+---------------------|
| I1    | Number  | =()()= → =()=  | Condense/Confirm    |
| I2    | Order   | =(())= → void  | Cancel/Compensate   |

- *I1 (Number)*: Multiple adjacent marks condense to a single mark
- *I2 (Order)*: A mark containing only a mark cancels to void (nothing)

Every well-formed expression reduces to exactly one of two values:
- =()= (the mark) — equivalent to TRUE or 1
- void (empty) — equivalent to FALSE or 0

* Setup

#+begin_src python
import os
import json
from dataclasses import dataclass, field
from dotenv import load_dotenv

from lofbench import generate_test_cases, generate_composite_test_cases
from lofbench.eval import quick_test, run_eval, analyze, SingleTask, CompositeTask

load_dotenv(override=True)

@dataclass
class Config:
    """Central configuration for all evaluations."""
    openai_key: str = field(default_factory=lambda: os.environ.get("OPENAI_API_KEY", ""))
    anthropic_key: str = field(default_factory=lambda: os.environ.get("ANTHROPIC_API_KEY", ""))
    gemini_key: str = field(default_factory=lambda: os.environ.get("GEMINI_API_KEY", ""))

    models: dict = field(default_factory=lambda: {
        "openai": "gpt-5.2",
        "anthropic": "claude-opus-4-5-20251101",
        "google": "gemini-3-pro-preview"
    })

    sample_size: int = 5
    seed: int = 2024

    @property
    def available_providers(self) -> list[str]:
        providers = []
        if self.openai_key: providers.append("openai")
        if self.anthropic_key: providers.append("anthropic")
        if self.gemini_key: providers.append("google")
        return providers

CFG = Config()

print("Setup complete.")
for p in ["openai", "anthropic", "google"]:
    key = getattr(CFG, f"{p}_key" if p != "google" else "gemini_key")
    status = "✓" if key else "✗"
    print(f"  {status} {p}: {CFG.models[p]}")
#+end_src

* Generate Test Cases

#+begin_src python
test_cases = generate_test_cases(n=4000, seed=20251211)
composite_cases = generate_composite_test_cases(n_groups=500, group_size=8, seed=2024)

print(f"Generated {len(test_cases)} single test cases")
print(f"Generated {len(composite_cases)} composite test cases")

with open("test_cases.json", "w") as f:
    json.dump(test_cases, f, indent=2)
with open("composite_test_cases.json", "w") as f:
    json.dump(composite_cases, f, indent=2)
#+end_src

* Load Test Cases

#+begin_src python
with open("test_cases.json", "r") as f:
    test_cases = json.load(f)
with open("composite_test_cases.json", "r") as f:
    composite_cases = json.load(f)
print(f"Loaded {len(test_cases)} single, {len(composite_cases)} composite test cases")
#+end_src

* Run Evaluation

** Quick Test (live calls)

#+begin_src python
results = quick_test(
    composite_cases,
    CompositeTask,
    n=CFG.sample_size,
    providers=CFG.available_providers,
    models=CFG.models,
)
with open("sample_results.json", "w") as f:
    json.dump(results, f, indent=2)
#+end_src

** Batch Run (batch APIs)

#+begin_src python
results = run_eval(
    composite_cases,
    CompositeTask,
    providers=CFG.available_providers,
    models=CFG.models,
    poll_interval=60,
)
with open("results.json", "w") as f:
    json.dump(results, f, indent=2)
print(f"Saved {len(results)} results")
#+end_src

* Analyze Results

#+begin_src python
with open("sample_results.json", "r") as f:
    data = json.load(f)
    results = data.get("results", data) if isinstance(data, dict) else data

if not results:
    print("No results to analyze")
else:
    analysis = analyze(results)
    print("=== Accuracy by Model ===")
    for model, stats in analysis["by_model"].items():
        pct = stats["accuracy"] * 100
        print(f"{model}: {pct:.1f}% ({stats['correct']}/{stats['total']})")
#+end_src
