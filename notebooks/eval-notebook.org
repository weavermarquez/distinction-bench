#+title: Distinction Benchmark
#+date: <2025-12-12 Fri>
#+author: Valerie Kim
#+property: header-args:python :session lof :results output :python "../.venv/bin/python"

* Introduction

This notebook evaluates whether Large Language Models can correctly simplify
Laws of Form (LoF) arithmetic expressions using Spencer-Brown's two axioms.

** The Calculus of Indications

George Spencer-Brown's /Laws of Form/ (1969) introduces a minimal calculus
based on a single symbol: the /mark/ or /cross/, written as =()=.

The calculus has only two axioms:

| Axiom | Name   | Rule        | Interpretation    |
|-------+--------+-------------+-------------------|
| I1    | Number | =()()= → =()=   | Condense/Confirm  |
| I2    | Order  | =(())= → void | Cancel/Compensate |

- *I1 (Number)*: Multiple adjacent marks condense to a single mark
- *I2 (Order)*: A mark containing only a mark cancels to void (nothing)

Every well-formed expression reduces to exactly one of two values:
- =()= (marked) — equivalent to TRUE or 1
- void (unmarked) — equivalent to FALSE or 0

* Setup

#+begin_src python
import os
import json
from dataclasses import dataclass, field
from dotenv import load_dotenv

from lofbench import generate_test_cases, generate_composite_test_cases
from lofbench.eval import quick_test, run_eval, analyze, SingleTask, CompositeTask

load_dotenv(override=True)

@dataclass
class Config:
    """Central configuration for all evaluations.

    Note: API keys here are only used to check provider availability.
    The SDKs read directly from environment variables:
      - OPENAI_API_KEY
      - ANTHROPIC_API_KEY
      - GOOGLE_API_KEY
    """
    openai_key: str = field(default_factory=lambda: os.environ.get("OPENAI_API_KEY", ""))
    anthropic_key: str = field(default_factory=lambda: os.environ.get("ANTHROPIC_API_KEY", ""))
    gemini_key: str = field(default_factory=lambda: os.environ.get("GOOGLE_API_KEY", ""))

    models: dict = field(default_factory=lambda: {
        "openai": "gpt-5.2",
        "anthropic": "claude-opus-4-5-20251101",
        "google": "gemini-3-pro-preview"
    })

    sample_size: int = 5
    seed: int = 2024

    @property
    def available_providers(self) -> list[str]:
        providers = []
        if self.openai_key: providers.append("openai")
        if self.anthropic_key: providers.append("anthropic")
        if self.gemini_key: providers.append("google")
        return providers

CFG = Config()

print("Setup complete.")
for p in ["openai", "anthropic", "google"]:
    key = getattr(CFG, f"{p}_key" if p != "google" else "gemini_key")
    status = "✓" if key else "✗"
    print(f"  {status} {p}: {CFG.models[p]}")
#+end_src

#+RESULTS:
: Setup complete.
:   ✓ openai: gpt-5.2
:   ✓ anthropic: claude-opus-4-5-20251101
:   ✓ google: gemini-3-pro-preview

* Generate Test Cases

#+begin_src python
test_cases = generate_test_cases(n=4000, seed=20251211)
composite_cases = generate_composite_test_cases(n_groups=500, group_size=8, seed=2024)

print(f"Generated {len(test_cases)} single test cases")
print(f"Generated {len(composite_cases)} composite test cases")

with open("test_cases.json", "w") as f:
    json.dump(test_cases, f, indent=2)
with open("composite_test_cases.json", "w") as f:
    json.dump(composite_cases, f, indent=2)
#+end_src

* Load Test Cases

#+begin_src python
with open("test_cases.json", "r") as f:
    test_cases = json.load(f)
with open("composite_test_cases.json", "r") as f:
    composite_cases = json.load(f)
print(f"Loaded {len(test_cases)} single, {len(composite_cases)} composite test cases")
#+end_src

#+RESULTS:
: Loaded 4000 single, 500 composite test cases

* Run Quick Test (live calls)

** Evaluate
#+begin_src python :async
results = quick_test(
    composite_cases,
    CompositeTask,
    n=CFG.sample_size,
    providers=CFG.available_providers,
    models=CFG.models,
)
with open("sample_results.json", "w") as f:
    json.dump(results, f, indent=2)
#+end_src

#+RESULTS:
#+begin_example
Results (15 cases):
  Per-item accuracy: 65.8%
  All-correct@8:     0/15 (0.0%)
  Count exact match: 1/15 (6.7%)
  ✗ comp_001 (openai): 3/8 items, count=5 (tgt 2)
  ✗ comp_001 (anthropic): 6/8 items, count=2 (tgt 2)
  ✗ comp_001 (google): 7/8 items, count=3 (tgt 2)
  ✗ comp_002 (openai): 7/8 items, count=8 (tgt 7)
  ✗ comp_002 (anthropic): 6/8 items, count=5 (tgt 7)
  ✗ comp_002 (google): 6/8 items, count=5 (tgt 7)
  ✗ comp_003 (openai): 5/8 items, count=4 (tgt 5)
  ✗ comp_003 (anthropic): 5/8 items, count=4 (tgt 5)
  ✗ comp_003 (google): 5/8 items, count=4 (tgt 5)
  ✗ comp_004 (openai): 3/8 items, count=4 (tgt 5)
  ✗ comp_004 (anthropic): 4/8 items, count=1 (tgt 5)
  ✗ comp_004 (google): 7/8 items, count=4 (tgt 5)
  ✗ comp_005 (openai): 4/8 items, count=7 (tgt 3)
  ✗ comp_005 (anthropic): 6/8 items, count=1 (tgt 3)
  ✗ comp_005 (google): 5/8 items, count=2 (tgt 3)
#+end_example

** Analyze Results

#+begin_src python
with open("sample_results.json", "r") as f:
    data = json.load(f)
    results = data.get("results", data) if isinstance(data, dict) else data

if not results:
    print("No results to analyze")
else:
    analysis = analyze(results)
    print("=== Results by Model ===")
    for model, stats in analysis["by_model"].items():
        print(f"{model} (n={stats['total']}):")
        print(f"  Accuracy:      {stats['accuracy']*100:.1f}% ({stats['correct']}/{stats['total']})")
        if "all_correct_rate" in stats:  # Composite metrics
            print(f"  All-correct:   {stats['all_correct_rate']*100:.1f}%")
            print(f"  Count match:   {stats['count_match_rate']*100:.1f}%")

    # Cost summary
    print("\n=== Cost by Model ===")
    from collections import defaultdict
    costs = defaultdict(float)
    for r in results:
        costs[r["metadata"]["model"]] += r["metadata"]["cost"]["total_cost"]
    total = sum(costs.values())
    for model, cost in sorted(costs.items()):
        print(f"  {model}: ${cost:.4f}")
    print(f"  TOTAL: ${total:.4f}")
#+end_src

#+RESULTS:
#+begin_example
=== Results by Model ===
gpt-5.2 (n=5):
  Accuracy:      55.0% (0/5)
  All-correct:   0.0%
  Count match:   0.0%
claude-opus-4-5-20251101 (n=5):
  Accuracy:      67.5% (0/5)
  All-correct:   0.0%
  Count match:   20.0%
gemini-3-pro-preview (n=5):
  Accuracy:      75.0% (0/5)
  All-correct:   0.0%
  Count match:   0.0%

=== Cost by Model ===
  claude-opus-4-5-20251101: $0.0997
  gemini-3-pro-preview: $0.0630
  gpt-5.2: $0.0066
  TOTAL: $0.1693
#+end_example

* Run Batch Run (batch APIs)

** Resume/Check Pending Batches

#+begin_src python :async
from lofbench.eval import resume_batch
results = resume_batch(
    composite_cases,
    CompositeTask,
    models=CFG.models,
    batch_log="batch_log.jsonl",
    poll_interval=30,
)
if results:
    with open("results.json", "w") as f:
        json.dump(results, f, indent=2)
    print(f"Saved {len(results)} results")
#+end_src

#+RESULTS:
: No pending batches, fetching completed results...
: Fetching results for: ['anthropic', 'google']
:   anthropic: fetched 500 results
: Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
:   google: fetched 500 results
: Saved 1000 results

** COMMENT Evaluate
#+begin_src python :async
results = run_eval(
    composite_cases,
    CompositeTask,
    providers=CFG.available_providers,
    models=CFG.models,
    poll_interval=30,
)
with open("results.json", "w") as f:
    json.dump(results, f, indent=2)
print(f"Saved {len(results)} results")
#+end_src

** Analyze Results

#+begin_src python
with open("results.json", "r") as f:
    data = json.load(f)
    results = data.get("results", data) if isinstance(data, dict) else data

if not results:
    print("No results to analyze")
else:
    analysis = analyze(results)
    print("=== Results by Model ===")
    for model, stats in analysis["by_model"].items():
        print(f"{model} (n={stats['total']}):")
        if "all_correct_rate" in stats:  # Composite metrics
            # Compute MAE for this model
            model_results = [r for r in results if r["metadata"]["model"] == model]
            valid = [r for r in model_results if r.get("answer_count", -1) >= 0]
            mae = sum(abs(r["answer_count"] - r["target_count"]) for r in valid) / len(valid) if valid else float('nan')
            print(f"  Per-item:      {stats['accuracy']*100:.1f}%")
            print(f"  All-correct:   {stats['all_correct_rate']*100:.1f}% ({stats['correct']}/{stats['total']})")
            print(f"  Count match:   {stats['count_match_rate']*100:.1f}%")
            print(f"  Count MAE:     {mae:.2f}")
        else:
            print(f"  Accuracy:      {stats['accuracy']*100:.1f}% ({stats['correct']}/{stats['total']})")

    # Results by difficulty
    # Compute avg steps per difficulty from single test cases
    from lofbench import simplify_string
    diff_steps = defaultdict(list)
    for r in results:
        for expr in r.get("expressions", [r.get("input")]):
            if expr:
                _, steps = simplify_string(expr)
                diff_steps[r["difficulty"]].append(len(steps))
    print("\n(average expected steps per difficulty)")
    for diff in sorted(diff_steps.keys()):
        avg = sum(diff_steps[diff]) / len(diff_steps[diff])
        print(f"  {diff:<12}: ~{avg:.1f} steps")
        
    print("\n=== Accuracy by Difficulty ===")
    for model, diffs in analysis["by_difficulty"].items():
        print(f"{model}:")
        for diff in sorted(diffs.keys()):
            stats = diffs[diff]
            if "all_correct_rate" in stats:
                print(f"  {diff:<12} {stats['accuracy']*100:5.1f}% per-item  {stats['all_correct_rate']*100:5.1f}% all-correct  (n={stats['total']})")
            else:
                print(f"  {diff:<12} {stats['accuracy']*100:5.1f}%  (n={stats['total']})")

    # Accuracy by target (marked vs unmarked)
    print("\n=== Accuracy by Target ===")
    by_model_target = defaultdict(lambda: defaultdict(lambda: {"correct": 0, "total": 0}))
    for r in results:
        model = r["metadata"]["model"]
        targets = r.get("target", [])
        answers = r.get("answer", [])
        if isinstance(targets, list):  # Composite
            for tgt, ans in zip(targets, answers):
                by_model_target[model][tgt]["total"] += 1
                if ans == tgt:
                    by_model_target[model][tgt]["correct"] += 1
        else:  # Single
            by_model_target[model][targets]["total"] += 1
            if answers == targets:
                by_model_target[model][targets]["correct"] += 1
    for model, targets in by_model_target.items():
        print(f"{model}:")
        for tgt in ["marked", "unmarked"]:
            stats = targets[tgt]
            if stats["total"] > 0:
                acc = stats["correct"] / stats["total"] * 100
                print(f"  {tgt:<12}: {acc:.1f}% ({stats['correct']}/{stats['total']})")

    # Random baselines (computed from actual target_count distribution)
    if results and "target_count" in results[0]:
        print("\n=== Random Baselines ===")
        true_counts = [r["target_count"] for r in results]
        n_results = len(true_counts)
        from collections import Counter
        count_dist = Counter(true_counts)
        # Histogram of target counts
        print("Target count distribution:")
        max_count = max(count_dist.values())
        for k in range(9):
            c = count_dist.get(k, 0)
            bar = "█" * int(40 * c / max_count) if max_count > 0 else ""
            print(f"  {k}: {bar} {c}")
        # Per-item: 50%
        print(f"\n  Per-item:       50.0% (random binary)")
        # All-correct: 0.5^8
        print(f"  All-correct:    {100 * 0.5**8:.2f}% (0.5^8)")
        # Count-match: P(guess from empirical dist matches) = sum P(k)^2
        p_match = sum((count_dist[k] / n_results) ** 2 for k in count_dist)
        print(f"  Count-match:    {100 * p_match:.1f}% (empirical dist)")
        # MAE baseline: E[|guess ~ empirical - true|]
        mae_baseline = sum(
            abs(g - tc) * count_dist[g] / n_results
            for tc in true_counts for g in count_dist
        ) / n_results
        print(f"  Count MAE:      {mae_baseline:.2f} (empirical dist)")

    # Cost summary
    print("\n=== Cost by Model ===")
    from collections import defaultdict
    costs = defaultdict(float)
    for r in results:
        costs[r["metadata"]["model"]] += r["metadata"]["cost"]["total_cost"]
    total = sum(costs.values())
    for model, cost in sorted(costs.items()):
        print(f"  {model}: ${cost:.4f}")
    print(f"  TOTAL: ${total:.4f}")
#+end_src

#+RESULTS:
#+begin_example
=== Results by Model ===
claude-opus-4-5-20251101 (n=500):
  Per-item:      62.8%
  All-correct:   4.0% (20/500)
  Count match:   14.6%
  Count MAE:     1.83
gemini-3-pro-preview (n=500):
  Per-item:      71.1%
  All-correct:   16.6% (83/500)
  Count match:   32.4%
  Count MAE:     1.15

(average expected steps per difficulty)
  1. easy     : ~3.3 steps
  2. medium   : ~8.6 steps
  3. hard     : ~12.7 steps
  4. lunatic  : ~14.7 steps
  5. extra    : ~18.1 steps

=== Accuracy by Difficulty ===
claude-opus-4-5-20251101:
  1. easy       76.4% per-item   14.0% all-correct  (n=100)
  2. medium     65.1% per-item    1.0% all-correct  (n=100)
  3. hard       54.0% per-item    2.0% all-correct  (n=100)
  4. lunatic    58.5% per-item    2.0% all-correct  (n=100)
  5. extra      60.0% per-item    1.0% all-correct  (n=100)
gemini-3-pro-preview:
  1. easy       91.1% per-item   56.0% all-correct  (n=100)
  2. medium     79.6% per-item   18.0% all-correct  (n=100)
  3. hard       55.4% per-item    0.0% all-correct  (n=100)
  4. lunatic    64.0% per-item    7.0% all-correct  (n=100)
  5. extra      65.5% per-item    2.0% all-correct  (n=100)

=== Accuracy by Target ===
claude-opus-4-5-20251101:
  marked      : 57.4% (1199/2089)
  unmarked    : 68.7% (1313/1911)
gemini-3-pro-preview:
  marked      : 70.8% (1479/2089)
  unmarked    : 71.5% (1366/1911)

=== Random Baselines ===
Target count distribution:
  0:  2
  1: ███ 24
  2: ████████████ 86
  3: ████████████████████████████ 198
  4: ████████████████████████████████████████ 282
  5: █████████████████████████████████ 236
  6: ██████████████████ 128
  7: █████ 40
  8:  4

  Per-item:       50.0% (random binary)
  All-correct:    0.39% (0.5^8)
  Count-match:    20.0% (empirical dist)
  Count MAE:      1.54 (empirical dist)

=== Cost by Model ===
  claude-opus-4-5-20251101: $0.0000
  gemini-3-pro-preview: $0.0000
  TOTAL: $0.0000
#+end_example
