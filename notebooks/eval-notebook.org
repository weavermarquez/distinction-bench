#+title: Distinction Benchmark
#+date: <2025-12-12 Fri>
#+author: Valerie Kim
#+property: header-args:python :session lof :results output :python "../.venv/bin/python"

* Introduction

This notebook evaluates whether Large Language Models can correctly simplify
Laws of Form (LoF) arithmetic expressions using Spencer-Brown's two axioms.

** The Calculus of Indications

George Spencer-Brown's /Laws of Form/ (1969) introduces a minimal calculus
based on a single symbol: the /mark/ or /cross/, written as =()=.

The calculus has only two axioms:

| Axiom | Name   | Rule        | Interpretation    |
|-------+--------+-------------+-------------------|
| I1    | Number | =()()= → =()=   | Condense/Confirm  |
| I2    | Order  | =(())= → void | Cancel/Compensate |

- *I1 (Number)*: Multiple adjacent marks condense to a single mark
- *I2 (Order)*: A mark containing only a mark cancels to void (nothing)

Every well-formed expression reduces to exactly one of two values:
- =()= (marked) — equivalent to TRUE or 1
- void (unmarked) — equivalent to FALSE or 0

* Setup

#+begin_src python
import os
import json
from dataclasses import dataclass, field
from dotenv import load_dotenv

from lofbench import generate_test_cases, generate_composite_test_cases
from lofbench.eval import quick_test, run_eval, analyze, SingleTask, CompositeTask

load_dotenv(override=True)

@dataclass
class Config:
    """Central configuration for all evaluations.

    Note: API keys here are only used to check provider availability.
    The SDKs read directly from environment variables:
      - OPENAI_API_KEY
      - ANTHROPIC_API_KEY
      - GOOGLE_API_KEY
    """
    openai_key: str = field(default_factory=lambda: os.environ.get("OPENAI_API_KEY", ""))
    anthropic_key: str = field(default_factory=lambda: os.environ.get("ANTHROPIC_API_KEY", ""))
    gemini_key: str = field(default_factory=lambda: os.environ.get("GOOGLE_API_KEY", ""))

    models: dict = field(default_factory=lambda: {
        "openai": "gpt-5.2",
        "anthropic": "claude-opus-4-5-20251101",
        "google": "gemini-3-pro-preview"
    })

    sample_size: int = 5
    seed: int = 2024

    @property
    def available_providers(self) -> list[str]:
        providers = []
        if self.openai_key: providers.append("openai")
        if self.anthropic_key: providers.append("anthropic")
        if self.gemini_key: providers.append("google")
        return providers

CFG = Config()

print("Setup complete.")
for p in ["openai", "anthropic", "google"]:
    key = getattr(CFG, f"{p}_key" if p != "google" else "gemini_key")
    status = "✓" if key else "✗"
    print(f"  {status} {p}: {CFG.models[p]}")
#+end_src

#+RESULTS:
: Setup complete.
:   ✓ openai: gpt-5.2
:   ✓ anthropic: claude-opus-4-5-20251101
:   ✓ google: gemini-3-pro-preview

* Generate Test Cases

#+begin_src python
test_cases = generate_test_cases(n=4000, seed=20251211)
composite_cases = generate_composite_test_cases(n_groups=500, group_size=8, seed=2024)

print(f"Generated {len(test_cases)} single test cases")
print(f"Generated {len(composite_cases)} composite test cases")

with open("test_cases.json", "w") as f:
    json.dump(test_cases, f, indent=2)
with open("composite_test_cases.json", "w") as f:
    json.dump(composite_cases, f, indent=2)
#+end_src

* Load Test Cases

#+begin_src python
with open("test_cases.json", "r") as f:
    test_cases = json.load(f)
with open("composite_test_cases.json", "r") as f:
    composite_cases = json.load(f)
print(f"Loaded {len(test_cases)} single, {len(composite_cases)} composite test cases")
#+end_src

#+RESULTS:
: Loaded 4000 single, 500 composite test cases

* Run Quick Test (live calls)

** Evaluate
#+begin_src python :async
results = quick_test(
    composite_cases,
    CompositeTask,
    n=CFG.sample_size,
    providers=CFG.available_providers,
    models=CFG.models,
)
with open("sample_results.json", "w") as f:
    json.dump(results, f, indent=2)
#+end_src

#+RESULTS:
#+begin_example
Results (15 cases):
  Per-item accuracy: 65.8%
  All-correct@8:     0/15 (0.0%)
  Count exact match: 1/15 (6.7%)
  ✗ comp_001 (openai): 3/8 items, count=5 (tgt 2)
  ✗ comp_001 (anthropic): 6/8 items, count=2 (tgt 2)
  ✗ comp_001 (google): 7/8 items, count=3 (tgt 2)
  ✗ comp_002 (openai): 7/8 items, count=8 (tgt 7)
  ✗ comp_002 (anthropic): 6/8 items, count=5 (tgt 7)
  ✗ comp_002 (google): 6/8 items, count=5 (tgt 7)
  ✗ comp_003 (openai): 5/8 items, count=4 (tgt 5)
  ✗ comp_003 (anthropic): 5/8 items, count=4 (tgt 5)
  ✗ comp_003 (google): 5/8 items, count=4 (tgt 5)
  ✗ comp_004 (openai): 3/8 items, count=4 (tgt 5)
  ✗ comp_004 (anthropic): 4/8 items, count=1 (tgt 5)
  ✗ comp_004 (google): 7/8 items, count=4 (tgt 5)
  ✗ comp_005 (openai): 4/8 items, count=7 (tgt 3)
  ✗ comp_005 (anthropic): 6/8 items, count=1 (tgt 3)
  ✗ comp_005 (google): 5/8 items, count=2 (tgt 3)
#+end_example

** Analyze Results

#+begin_src python
with open("sample_results.json", "r") as f:
    data = json.load(f)
    results = data.get("results", data) if isinstance(data, dict) else data

if not results:
    print("No results to analyze")
else:
    analysis = analyze(results)
    print("=== Results by Model ===")
    for model, stats in analysis["by_model"].items():
        print(f"{model} (n={stats['total']}):")
        print(f"  Accuracy:      {stats['accuracy']*100:.1f}% ({stats['correct']}/{stats['total']})")
        if "all_correct_rate" in stats:  # Composite metrics
            print(f"  All-correct:   {stats['all_correct_rate']*100:.1f}%")
            print(f"  Count match:   {stats['count_match_rate']*100:.1f}%")

    # Cost summary
    print("\n=== Cost by Model ===")
    from collections import defaultdict
    costs = defaultdict(float)
    for r in results:
        costs[r["metadata"]["model"]] += r["metadata"]["cost"]["total_cost"]
    total = sum(costs.values())
    for model, cost in sorted(costs.items()):
        print(f"  {model}: ${cost:.4f}")
    print(f"  TOTAL: ${total:.4f}")
#+end_src

#+RESULTS:
#+begin_example
=== Results by Model ===
gpt-5.2 (n=5):
  Accuracy:      55.0% (0/5)
  All-correct:   0.0%
  Count match:   0.0%
claude-opus-4-5-20251101 (n=5):
  Accuracy:      67.5% (0/5)
  All-correct:   0.0%
  Count match:   20.0%
gemini-3-pro-preview (n=5):
  Accuracy:      75.0% (0/5)
  All-correct:   0.0%
  Count match:   0.0%

=== Cost by Model ===
  claude-opus-4-5-20251101: $0.0997
  gemini-3-pro-preview: $0.0630
  gpt-5.2: $0.0066
  TOTAL: $0.1693
#+end_example

* Run Batch Run (batch APIs)

** Resume/Check Pending Batches

#+begin_src python :async
from lofbench.eval import resume_batch
results = resume_batch(
    composite_cases,
    CompositeTask,
    models=CFG.models,
    batch_log="batch_log.jsonl",
    poll_interval=30,
)
if results:
    with open("results.json", "w") as f:
        json.dump(results, f, indent=2)
    print(f"Saved {len(results)} results")
#+end_src

#+RESULTS:
: No pending batches, fetching completed results...
: Fetching results for: ['anthropic', 'google']
:   anthropic: fetched 500 results
: Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
:   google: fetched 500 results
: Saved 1000 results

** COMMENT Evaluate
#+begin_src python :async
results = run_eval(
    composite_cases,
    CompositeTask,
    providers=CFG.available_providers,
    models=CFG.models,
    poll_interval=30,
)
with open("results.json", "w") as f:
    json.dump(results, f, indent=2)
print(f"Saved {len(results)} results")
#+end_src

** Analyze Results

#+begin_src python
from lofbench.eval import print_dataset_stats, print_eval_results
import json

with open("results.json", "r") as f:
    data = json.load(f)
    results = data.get("results", data) if isinstance(data, dict) else data

print_dataset_stats(composite_cases)
print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
print_eval_results(results)
#+end_src

#+RESULTS:
#+begin_example
=== Target Count Distribution ===
  0:  1
  1: ███ 12
  2: ████████████ 43
  3: ████████████████████████████ 99
  4: ████████████████████████████████████████ 141
  5: █████████████████████████████████ 118
  6: ██████████████████ 64
  7: █████ 20
  8:  2

=== Random Baselines ===
  Per-item:       50.0% (random binary)
  All-correct:    0.39% (0.5^8)
  Count-match:    20.0% (empirical dist)
  Count MAE:      1.54 (empirical dist)

=== Average Steps per Difficulty ===
  1. easy     : ~3.3 steps
  2. medium   : ~8.6 steps
  3. hard     : ~12.7 steps
  4. lunatic  : ~14.7 steps
  5. extra    : ~18.1 steps
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
=== Results by Model ===
claude-opus-4-5-20251101 (n=500):
  Per-item:      62.8%
  All-correct:   4.0% (20/500)
  Count match:   14.6%
  Count MAE:     1.83
gemini-3-pro-preview (n=500):
  Per-item:      71.1%
  All-correct:   16.6% (83/500)
  Count match:   32.4%
  Count MAE:     1.15

=== Accuracy by Difficulty ===
claude-opus-4-5-20251101:
  1. easy       76.4% per-item   14.0% all-correct  (n=100)
  2. medium     65.1% per-item    1.0% all-correct  (n=100)
  3. hard       54.0% per-item    2.0% all-correct  (n=100)
  4. lunatic    58.5% per-item    2.0% all-correct  (n=100)
  5. extra      60.0% per-item    1.0% all-correct  (n=100)
gemini-3-pro-preview:
  1. easy       91.1% per-item   56.0% all-correct  (n=100)
  2. medium     79.6% per-item   18.0% all-correct  (n=100)
  3. hard       55.4% per-item    0.0% all-correct  (n=100)
  4. lunatic    64.0% per-item    7.0% all-correct  (n=100)
  5. extra      65.5% per-item    2.0% all-correct  (n=100)

=== Accuracy by Target ===
claude-opus-4-5-20251101:
  marked      : 57.4% (1199/2089)
  unmarked    : 68.7% (1313/1911)
gemini-3-pro-preview:
  marked      : 70.8% (1479/2089)
  unmarked    : 71.5% (1366/1911)
#+end_example
