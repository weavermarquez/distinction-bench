#+title: Distinction Benchmark
#+date: <2025-12-12 Fri>
#+author: Valerie Kim
#+property: header-args:python :session lof :results output :python "../.venv/bin/python"

* Introduction

This notebook evaluates whether Large Language Models can correctly simplify
Laws of Form (LoF) arithmetic expressions using Spencer-Brown's two axioms.

** The Calculus of Indications

George Spencer-Brown's /Laws of Form/ (1969) introduces a minimal calculus
based on a single symbol: the /mark/ or /cross/, written as =()=.

The calculus has only two axioms:

| Axiom | Name   | Rule        | Interpretation    |
|-------+--------+-------------+-------------------|
| I1    | Number | =()()= → =()=   | Condense/Confirm  |
| I2    | Order  | =(())= → void | Cancel/Compensate |

- *I1 (Number)*: Multiple adjacent marks condense to a single mark
- *I2 (Order)*: A mark containing only a mark cancels to void (nothing)

Every well-formed expression reduces to exactly one of two values:
- =()= (marked) — equivalent to TRUE or 1
- void (unmarked) — equivalent to FALSE or 0

* Setup

#+begin_src python
import os
import json
from dataclasses import dataclass, field
from dotenv import load_dotenv

from lofbench import generate_test_cases, generate_composite_test_cases
from lofbench.eval import quick_test, run_eval, analyze, SingleTask, CompositeTask

load_dotenv(override=True)

@dataclass
class Config:
    """Central configuration for all evaluations.

    Note: API keys here are only used to check provider availability.
    The SDKs read directly from environment variables:
      - OPENAI_API_KEY
      - ANTHROPIC_API_KEY
      - GOOGLE_API_KEY
    """
    openai_key: str = field(default_factory=lambda: os.environ.get("OPENAI_API_KEY", ""))
    anthropic_key: str = field(default_factory=lambda: os.environ.get("ANTHROPIC_API_KEY", ""))
    gemini_key: str = field(default_factory=lambda: os.environ.get("GOOGLE_API_KEY", ""))

    models: dict = field(default_factory=lambda: {
        "openai": "gpt-5.2",
        "anthropic": "claude-opus-4-5-20251101",
        "google": "gemini-3-pro-preview"
    })

    sample_size: int = 5
    seed: int = 2024

    @property
    def available_providers(self) -> list[str]:
        providers = []
        if self.openai_key: providers.append("openai")
        if self.anthropic_key: providers.append("anthropic")
        if self.gemini_key: providers.append("google")
        return providers

CFG = Config()

print("Setup complete.")
for p in ["openai", "anthropic", "google"]:
    key = getattr(CFG, f"{p}_key" if p != "google" else "gemini_key")
    status = "✓" if key else "✗"
    print(f"  {status} {p}: {CFG.models[p]}")
#+end_src

#+RESULTS:
: Setup complete.
:   ✓ openai: gpt-5.2
:   ✓ anthropic: claude-opus-4-5-20251101
:   ✓ google: gemini-3-pro-preview

* Generate Test Cases

#+begin_src python
test_cases = generate_test_cases(n=4000, seed=20251211)
composite_cases = generate_composite_test_cases(n_groups=500, group_size=8, seed=2024)

print(f"Generated {len(test_cases)} single test cases")
print(f"Generated {len(composite_cases)} composite test cases")

with open("test_cases.json", "w") as f:
    json.dump(test_cases, f, indent=2)
with open("composite_test_cases.json", "w") as f:
    json.dump(composite_cases, f, indent=2)
#+end_src

* Load Test Cases

#+begin_src python
with open("test_cases.json", "r") as f:
    test_cases = json.load(f)
with open("composite_test_cases.json", "r") as f:
    composite_cases = json.load(f)
print(f"Loaded {len(test_cases)} single, {len(composite_cases)} composite test cases")
#+end_src

* Run Quick Test (live calls)

** Evaluate
#+begin_src python :async
results = quick_test(
    composite_cases,
    CompositeTask,
    n=CFG.sample_size,
    providers=CFG.available_providers,
    models=CFG.models,
)
with open("sample_results.json", "w") as f:
    json.dump(results, f, indent=2)
#+end_src

** Analyze Results

#+begin_src python
with open("sample_results.json", "r") as f:
    data = json.load(f)
    results = data.get("results", data) if isinstance(data, dict) else data

if not results:
    print("No results to analyze")
else:
    analysis = analyze(results)
    print("=== Results by Model ===")
    for model, stats in analysis["by_model"].items():
        print(f"{model} (n={stats['total']}):")
        print(f"  Accuracy:      {stats['accuracy']*100:.1f}% ({stats['correct']}/{stats['total']})")
        if "all_correct_rate" in stats:  # Composite metrics
            print(f"  All-correct:   {stats['all_correct_rate']*100:.1f}%")
            print(f"  Count match:   {stats['count_match_rate']*100:.1f}%")
#+end_src

#+RESULTS:
: === Accuracy by Model ===
: gpt-5.2: 47.5% (0/5)
: claude-opus-4-5-20251101: 62.5% (1/5)
: gemini-3-pro-preview: 72.5% (1/5)

* COMMENT Run Batch Run (batch APIs)

** Evaluate
#+begin_src python :async
results = run_eval(
    composite_cases,
    CompositeTask,
    providers=CFG.available_providers,
    models=CFG.models,
    poll_interval=60,
)
with open("results.json", "w") as f:
    json.dump(results, f, indent=2)
print(f"Saved {len(results)} results")
#+end_src

** Analyze Results

#+begin_src python
with open("results.json", "r") as f:
    data = json.load(f)
    results = data.get("results", data) if isinstance(data, dict) else data

if not results:
    print("No results to analyze")
else:
    analysis = analyze(results)
    print("=== Results by Model ===")
    for model, stats in analysis["by_model"].items():
        print(f"{model} (n={stats['total']}):")
        print(f"  Accuracy:      {stats['accuracy']*100:.1f}% ({stats['correct']}/{stats['total']})")
        if "all_correct_rate" in stats:  # Composite metrics
            print(f"  All-correct:   {stats['all_correct_rate']*100:.1f}%")
            print(f"  Count match:   {stats['count_match_rate']*100:.1f}%")
#+end_src
